{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"DKcIdYD2sySs"},"source":["# Bert\n","作成者：長谷川\n","- Bertを用いて文章分類  \n","LSTMと同じデータを用いて精度を比較してみましょう"]},{"cell_type":"markdown","source":["## data"],"metadata":{"id":"-CkdVoTckfc0"}},{"cell_type":"code","metadata":{"id":"0hJ-pXOwXBzH"},"source":["# 必要なライブラリのインストール\n","!pip install transformers==4.18.0 fugashi==1.1.0 ipadic==1.0.0 pytorch-lightning==1.6.1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_BGiKTflI39"},"source":["import random\n","import glob\n","from tqdm import tqdm\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from transformers import BertJapaneseTokenizer, BertForSequenceClassification\n","import pytorch_lightning as pl\n","\n","# 日本語の事前学習モデル\n","MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r97ZbgVeZ-Hi"},"source":["#データのダウンロード\n","!wget https://www.rondhuit.com/download/ldcc-20140209.tar.gz \n","#ファイルの解凍\n","!tar -zxf ldcc-20140209.tar.gz "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TMUJ3rscgG2z"},"source":["!cat ./text/it-life-hack/it-life-hack-6342280.txt # ファイルを表示"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 前処理"],"metadata":{"id":"Agnor-xPqzrc"}},{"cell_type":"code","metadata":{"id":"G9YGEfZUAxea"},"source":["# カテゴリーのリスト\n","category_list = [\n","    'dokujo-tsushin',\n","    'it-life-hack',\n","    'kaden-channel',\n","    'livedoor-homme',\n","    'movie-enter',\n","    'peachy',\n","    'smax',\n","    'sports-watch',\n","    'topic-news'\n","]\n","\n","# トークナイザのロード\n","tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n","\n","# 各データの形式を整える\n","max_length = 128\n","dataset_for_loader = []\n","for label, category in enumerate(tqdm(category_list)):\n","    for file in glob.glob(f'./text/{category}/{category}*'):\n","        lines = open(file).read().splitlines()\n","        text = '\\n'.join(lines[3:]) # ファイルの4行目からを抜き出す。\n","        encoding = tokenizer(\n","            text,\n","            max_length=max_length, \n","            padding='max_length',\n","            truncation=True\n","        )\n","        encoding['labels'] = label # ラベルを追加\n","        encoding = { k: torch.tensor(v) for k, v in encoding.items() }\n","        dataset_for_loader.append(encoding)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"drP8IYLVBFh_"},"source":["print(dataset_for_loader[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XHY9Os6NJlip"},"source":["# データセットの分割\n","random.shuffle(dataset_for_loader) # ランダムにシャッフル\n","n = len(dataset_for_loader)\n","n_train = int(0.6*n)\n","n_val = int(0.2*n)\n","dataset_train = dataset_for_loader[:n_train] # 学習データ\n","dataset_val = dataset_for_loader[n_train:n_train+n_val] # 検証データ\n","dataset_test = dataset_for_loader[n_train+n_val:] # テストデータ\n","\n","# データセットからデータローダを作成\n","# 学習データはshuffle=Trueにする。\n","dataloader_train = DataLoader(\n","    dataset_train, batch_size=32, shuffle=True\n",") \n","dataloader_val = DataLoader(dataset_val, batch_size=256)\n","dataloader_test = DataLoader(dataset_test, batch_size=256)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## model"],"metadata":{"id":"bv-BqHCvrTWv"}},{"cell_type":"code","metadata":{"id":"ffaUyGcoVj8l"},"source":["class BertForSequenceClassification_pl(pl.LightningModule):\n","        \n","    def __init__(self, model_name, num_labels, lr):\n","        # model_name: Transformersのモデルの名前\n","        # num_labels: ラベルの数\n","        # lr: 学習率\n","\n","        super().__init__()\n","        \n","        # 引数のnum_labelsとlrを保存。\n","        # 例えば、self.hparams.lrでlrにアクセスできる。\n","        # チェックポイント作成時にも自動で保存される。\n","        self.save_hyperparameters() \n","\n","        # BERTのロード\n","        self.bert_sc = BertForSequenceClassification.from_pretrained(\n","            model_name,\n","            num_labels=num_labels\n","        )\n","        \n","    # 学習データのミニバッチ(`batch`)が与えられた時に損失を出力する関数を書く。\n","    # batch_idxはミニバッチの番号であるが今回は使わない。\n","    def training_step(self, batch, batch_idx):\n","        output = self.bert_sc(**batch)\n","        loss = output.loss\n","        self.log('train_loss', loss) # 損失を'train_loss'の名前でログをとる。\n","        return loss\n","        \n","    # 検証データのミニバッチが与えられた時に、\n","    # 検証データを評価する指標を計算する関数を書く。\n","    def validation_step(self, batch, batch_idx):\n","        output = self.bert_sc(**batch)\n","        val_loss = output.loss\n","        self.log('val_loss', val_loss) # 損失を'val_loss'の名前でログをとる。\n","\n","    # テストデータのミニバッチが与えられた時に、\n","    # テストデータを評価する指標を計算する関数を書く。\n","    def test_step(self, batch, batch_idx):\n","        labels = batch.pop('labels') # バッチからラベルを取得\n","        output = self.bert_sc(**batch)\n","        labels_predicted = output.logits.argmax(-1)\n","        num_correct = ( labels_predicted == labels ).sum().item()\n","        accuracy = num_correct/labels.size(0) #精度\n","        self.log('accuracy', accuracy) # 精度を'accuracy'の名前でログをとる。\n","\n","    # 学習に用いるオプティマイザを返す関数を書く。\n","    def configure_optimizers(self):\n","        return torch.optim.Adam(self.parameters(), lr=self.hparams.lr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 学習"],"metadata":{"id":"u7bqYO5GwJH3"}},{"cell_type":"code","metadata":{"id":"lyR6de1TqfW9"},"source":["# 学習時にモデルの重みを保存する条件を指定\n","checkpoint = pl.callbacks.ModelCheckpoint(\n","    monitor='val_loss',\n","    mode='min',\n","    save_top_k=1,\n","    save_weights_only=True,\n","    dirpath='model/',\n",")\n","\n","# 学習の方法を指定\n","trainer = pl.Trainer(\n","    gpus=1, \n","    max_epochs=10,\n","    callbacks = [checkpoint]\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgk48zEqIJKh"},"source":["# PyTorch Lightningモデルのロード\n","model = BertForSequenceClassification_pl(\n","    MODEL_NAME, num_labels=9, lr=1e-5\n",")\n","\n","# ファインチューニングを行う。\n","trainer.fit(model, dataloader_train, dataloader_val) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h68P7MG-JSh9"},"source":["best_model_path = checkpoint.best_model_path # ベストモデルのファイル\n","print('ベストモデルのファイル: ', checkpoint.best_model_path)\n","print('ベストモデルの検証データに対する損失: ', checkpoint.best_model_score)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A-r9stqZqBdW"},"source":["%load_ext tensorboard\n","%tensorboard --logdir ./"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ib_-F0qrwUIb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["def __init__(self, model_name, num_labels, lr):## 予想精度確認"],"metadata":{"id":"bcpwscH3wUfm"}},{"cell_type":"code","metadata":{"id":"6bx0L0Ehr1tM"},"source":["test = trainer.test(dataloaders=dataloader_test)\n","print(f'Accuracy: {test[0][\"accuracy\"]:.2f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mQkU9GqWw4c0"},"execution_count":null,"outputs":[]}]}